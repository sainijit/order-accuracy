version: "3.8"

services:
  minio:
    image: minio/minio
    container_name: oa_minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      NO_PROXY: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
      no_proxy: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    restart: unless-stopped
    networks:
      - order-accuracy-net

  # OVMS VLM service - automatically started when VLM_BACKEND=ovms
  # To use OVMS backend:
  # 1. Run: cd ovms-service && ./setup_models.sh
  # 2. Set VLM_BACKEND=ovms in application-service environment below
  # 3. Run: docker-compose up --build -d
  ovms-vlm:
    image: openvino/model_server:latest-gpu  # GPU-optimized image
    container_name: oa_ovms_vlm
    ports:
      - "8001:8000"
    volumes:
      - ./models:/models:ro
      - ./ovms-service/cache:/tmp/ov_cache  # Model compilation cache for faster startup
    environment:
      - OV_CACHE_DIR=/tmp/ov_cache
    command:
      - --rest_port=8000
      - --config_path=/models/config.json
      - --log_level=INFO
    # GPU device access
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - "44"   # video group for GPU access
      - "992"  # render group for GPU access
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - order-accuracy-net
    profiles:
      - ovms  # Only start when --profile ovms is used

  application-service:
    build:
      context: ./application-service
      args:
        http_proxy: http://proxy-pilot.intel.com:912
        https_proxy: http://proxy-pilot.intel.com:912
        HTTP_PROXY: http://proxy-pilot.intel.com:912
        HTTPS_PROXY: http://proxy-pilot.intel.com:912
        no_proxy: localhost,127.0.0.1
        NO_PROXY: localhost,127.0.0.1
    container_name: oa_application
    # Remove GPU device access when using OVMS backend
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - video
    volumes:
      - ./storage/videos:/videos
      - ./storage/uploads:/uploads
      - ./model:/model  # Only needed for embedded backend
      - ./config:/config:ro
    environment:
      APP_CONFIG: /config/application.yaml
      
      # Backend selection: 'embedded' or 'ovms'
      VLM_BACKEND: embedded  # Change to 'ovms' to use OVMS backend
      
      # Embedded backend settings (used when VLM_BACKEND=embedded)
      VLM_MODEL_PATH: /model/Qwen2.5-VL-7B-Instruct-ov-int8
      OPENVINO_DEVICE: GPU
      
      # OVMS backend settings (used when VLM_BACKEND=ovms)
      OVMS_ENDPOINT: http://ovms-vlm:8000
      OVMS_MODEL_NAME: Qwen/Qwen2.5-VL-7B-Instruct-ov-int8
      
      # Semantic Comparison Service settings
      SEMANTIC_SERVICE_ENDPOINT: http://semantic-service:8080
      USE_SEMANTIC_SERVICE: "true"  # Enable external semantic comparison service
      
      # CPU thread limits
      OMP_NUM_THREADS: 4
      MKL_NUM_THREADS: 4
      OPENBLAS_NUM_THREADS: 4
      NO_PROXY: "localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,host.docker.internal"
      no_proxy: "localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,host.docker.internal"
    ports:
      - "8000:8000"
    depends_on:
      - minio
    extra_hosts:
      - "host.docker.internal:host-gateway"
    cpus: 4
    shm_size: 2g
    networks:
      - order-accuracy-net

  frame-selector:
    build:
      context: ./frame-selector-service
      args:
        http_proxy: http://proxy-pilot.intel.com:912
        https_proxy: http://proxy-pilot.intel.com:912
        HTTP_PROXY: http://proxy-pilot.intel.com:912
        HTTPS_PROXY: http://proxy-pilot.intel.com:912
        no_proxy: localhost,127.0.0.1
        NO_PROXY: localhost,127.0.0.1
    container_name: oa_frame_selector
    volumes:
      - ./config:/config:ro
      - ./model:/app/models
      - ./datasets:/app/datasets
    environment:
      APP_CONFIG: /config/application.yaml
      http_proxy: http://proxy-pilot.intel.com:912
      https_proxy: http://proxy-pilot.intel.com:912
      HTTP_PROXY: http://proxy-pilot.intel.com:912
      HTTPS_PROXY: http://proxy-pilot.intel.com:912
      NO_PROXY: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
      no_proxy: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
    depends_on:
      - application-service
    networks:
      - order-accuracy-net

  gradio-ui:
    build:
      context: ./gradio-ui
      args:
        http_proxy: http://proxy-pilot.intel.com:912
        https_proxy: http://proxy-pilot.intel.com:912
        HTTP_PROXY: http://proxy-pilot.intel.com:912
        HTTPS_PROXY: http://proxy-pilot.intel.com:912
        no_proxy: localhost,127.0.0.1
        NO_PROXY: localhost,127.0.0.1
    container_name: oa_gradio
    ports:
      - "7860:7860"
    depends_on:
      - application-service
    environment:
      NO_PROXY: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
      no_proxy: "localhost,127.0.0.1,application-service,minio,ovms-vlm,host.docker.internal"
    networks:
      - order-accuracy-net

  # Semantic Comparison Service - AI-powered item matching
  semantic-service:
    image: semantic-comparison-service:latest
    container_name: oa_semantic_service
    ports:
      - "8080:8080"
      - "9090:9090"  # Prometheus metrics
    volumes:
      - ./dine-in/orders:/app/config:ro  # Mount dine-in orders and inventory files
    environment:
      - SERVICE_NAME=semantic-comparison-service
      - LOG_LEVEL=INFO
      - VLM_BACKEND=ovms
      - OVMS_ENDPOINT=http://ovms-vlm:8000
      - OVMS_MODEL_NAME=Qwen/Qwen2-VL-2B-Instruct
      - CACHE_ENABLED=true
      - CACHE_BACKEND=memory
      - PROMETHEUS_ENABLED=true
      - CONFIG_DIR=/app/config
      - ORDERS_FILE=/app/config/orders.json
      - INVENTORY_FILE=/app/config/inventory.json
      - NO_PROXY=localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,host.docker.internal
      - no_proxy=localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,host.docker.internal
    depends_on:
      - ovms-vlm
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v1/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - order-accuracy-net
    profiles:
      - ovms  # Only start when using OVMS backend

  # RTSP Streamer - Converts video files to RTSP streams
  rtsp-streamer:
    build:
      context: ./rtsp-streamer
      args:
        http_proxy: http://proxy-pilot.intel.com:912
        https_proxy: http://proxy-pilot.intel.com:912
        HTTP_PROXY: http://proxy-pilot.intel.com:912
        HTTPS_PROXY: http://proxy-pilot.intel.com:912
        no_proxy: localhost,127.0.0.1
        NO_PROXY: localhost,127.0.0.1
    container_name: oa_rtsp_streamer
    volumes:
      - ./storage/videos:/media:ro  # Mount videos as read-only
    environment:
      - MEDIA_DIR=/media
      - RTSP_PORT=8554
    ports:
      - "8554:8554"  # RTSP port
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "127.0.0.1", "8554"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - order-accuracy-net
    profiles:
      - parallel  # Start with parallel-pipeline

  # Parallel Pipeline Manager - Multi-station processing with autoscaling
  parallel-pipeline:
    build:
      context: .  # Build from root to access application-service
      dockerfile: ./parallel-pipeline/Dockerfile
      args:
        http_proxy: http://proxy-pilot.intel.com:912
        https_proxy: http://proxy-pilot.intel.com:912
        HTTP_PROXY: http://proxy-pilot.intel.com:912
        HTTPS_PROXY: http://proxy-pilot.intel.com:912
        no_proxy: localhost,127.0.0.1
        NO_PROXY: localhost,127.0.0.1
    container_name: oa_parallel_pipeline
    depends_on:
      rtsp-streamer:
        condition: service_healthy
      minio:
        condition: service_started
    volumes:
      - ./parallel-pipeline:/app:rw  # Mount parallel-pipeline code first
      - ./storage/videos:/videos
      - ./application-service/app:/app/application-service/app:ro
      - ./frame-selector-service/app:/app/frame-selector-service/app:ro
      - ./config:/config:ro
      - ./model:/model:ro
      - ./datasets:/datasets:ro  # Changed to root /datasets instead of /app/datasets
      - ./parallel-pipeline/config:/app/config:rw
    environment:
      - VLM_BACKEND=ovms
      - OVMS_ENDPOINT=http://ovms-vlm:8000
      - SEMANTIC_SERVICE_ENDPOINT=http://semantic-service:8080
      - USE_SEMANTIC_SERVICE=true
      - PIPELINE_TEST_MODE=true
      - TEST_ORDER_DURATION=30
      - RTSP_STREAMER_HOST=rtsp-streamer
      - RTSP_STREAMER_PORT=8554
      - NO_PROXY=localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,rtsp-streamer,host.docker.internal
      - no_proxy=localhost,127.0.0.1,application-service,minio,ovms-vlm,semantic-service,rtsp-streamer,host.docker.internal
    command: ["python3", "main.py", "--log-level", "INFO", "fixed", "--stations", "2"]
    restart: unless-stopped
    networks:
      - order-accuracy-net
    profiles:
      - parallel  # Optional profile - start with: docker-compose --profile parallel up

networks:
  order-accuracy-net:
    driver: bridge

volumes:
  minio_data: